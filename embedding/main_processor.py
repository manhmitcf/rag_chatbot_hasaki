#!/usr/bin/env python3

"""
Main processor cho h·ªá th·ªëng embedding n√¢ng cao v·ªõi MarkdownTextSplitter
"""

import json
import time
import uuid
from typing import List, Dict, Any
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct
import os
from config import (
    QDRANT_HOST, QDRANT_PORT, QDRANT_COLLECTION_NAME,
    EMBEDDING_MODEL, EMBEDDING_DIMENSION,
    DEFAULT_CHUNK_SIZE, DEFAULT_OVERLAP,
    MARKDOWN_CHUNK_SIZE, MARKDOWN_OVERLAP,
    DEVICE, EMBEDDING_BATCH_SIZE, MAX_PRODUCTS_LIMIT
)
from data_loader import load_and_validate_data
from product_processor import process_products_with_advanced_chunking

class AdvancedEmbeddingPipeline:
    """
    Pipeline x·ª≠ l√Ω embedding n√¢ng cao
    """
    
    def __init__(self, 
                 qdrant_host: str = QDRANT_HOST,
                 qdrant_port: int = QDRANT_PORT,
                 collection_name: str = QDRANT_COLLECTION_NAME,
                 embedding_model: str = EMBEDDING_MODEL):
        """
        Kh·ªüi t·∫°o pipeline
        
        Args:
            qdrant_host: Host c·ªßa Qdrant
            qdrant_port: Port c·ªßa Qdrant
            collection_name: T√™n collection
            embedding_model: T√™n model embedding
        """
        self.qdrant_host = qdrant_host
        self.qdrant_port = qdrant_port
        self.collection_name = collection_name
        self.embedding_model = embedding_model
        self.model = None
        self.qdrant_client = None
    
    def load_model(self):
        """T·∫£i model embedding v·ªõi GPU support"""
        print(f"ƒêang t·∫£i model: {self.embedding_model}")
        print(f"Device: {DEVICE}")
        start_time = time.time()
        
        # T·∫£i model
        self.model = SentenceTransformer(self.embedding_model, device=DEVICE)
        
        # Hi·ªÉn th·ªã th√¥ng tin GPU n·∫øu c√≥
        if DEVICE.startswith("cuda"):
            import torch
            gpu_memory = torch.cuda.get_device_properties(DEVICE).total_memory / 1024**3
            print(f"GPU Memory: {gpu_memory:.1f} GB")
            print(f"Embedding batch size: {EMBEDDING_BATCH_SIZE}")
        
        load_time = time.time() - start_time
        print(f"Model ƒë√£ ƒë∆∞·ª£c t·∫£i trong {load_time:.2f} gi√¢y")
    
    def connect_qdrant(self):
        """K·∫øt n·ªëi t·ªõi Qdrant"""
        try:
            print(f"ƒêang k·∫øt n·ªëi t·ªõi Qdrant: {self.qdrant_host}:{self.qdrant_port}")
            
            self.qdrant_client = QdrantClient(
                host=self.qdrant_host,
                port=self.qdrant_port,
                timeout=60.0
            )
            
            # Test connection
            collections = self.qdrant_client.get_collections()
            print(f"K·∫øt n·ªëi th√†nh c√¥ng! C√≥ {len(collections.collections)} collections")
            
        except Exception as e:
            print(f"L·ªói k·∫øt n·ªëi Qdrant: {str(e)}")
            raise e
    
    def setup_collection(self, recreate: bool = True):
        """
        Thi·∫øt l·∫≠p collection trong Qdrant
        
        Args:
            recreate: C√≥ t·∫°o l·∫°i collection kh√¥ng
        """
        try:
            # Ki·ªÉm tra collection t·ªìn t·∫°i
            if self.qdrant_client.collection_exists(self.collection_name):
                if recreate:
                    print(f"Collection '{self.collection_name}' ƒë√£ t·ªìn t·∫°i. ƒêang x√≥a...")
                    self.qdrant_client.delete_collection(self.collection_name)
                    print("ƒê√£ x√≥a collection c≈©")
                else:
                    print(f"Collection '{self.collection_name}' ƒë√£ t·ªìn t·∫°i. S·ª≠ d·ª•ng collection hi·ªán t·∫°i.")
                    return
            
            # T·∫°o collection m·ªõi
            print(f"ƒêang t·∫°o collection m·ªõi: {self.collection_name}")
            self.qdrant_client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=EMBEDDING_DIMENSION,
                    distance=Distance.COSINE
                ),
                shard_number=1
            )
            print("ƒê√£ t·∫°o collection th√†nh c√¥ng")
            
        except Exception as e:
            print(f"L·ªói khi thi·∫øt l·∫≠p collection: {str(e)}")
            raise e
    
    def upload_embeddings(self, embeddings: List[Dict[str, Any]], batch_size: int = 100):
        """
        Upload embeddings l√™n Qdrant
        
        Args:
            embeddings: Danh s√°ch embeddings
            batch_size: K√≠ch th∆∞·ªõc batch
        """
        try:
            print(f"ƒêang upload {len(embeddings)} embeddings v·ªõi batch size {batch_size}")
            
            for i in tqdm(range(0, len(embeddings), batch_size), desc="Uploading"):
                batch_docs = embeddings[i:i + batch_size]
                batch_points = []
                
                for doc in batch_docs:
                    try:
                        point = PointStruct(
                            id=str(uuid.uuid4()),
                            vector=doc["values"],
                            payload=doc["metadata"]
                        )
                        batch_points.append(point)
                    except Exception as e:
                        print(f"L·ªói t·∫°o point: {str(e)}")
                        continue
                
                if batch_points:
                    self.qdrant_client.upsert(
                        collection_name=self.collection_name,
                        points=batch_points
                    )
            
            print("Upload ho√†n th√†nh!")
            
            # Ki·ªÉm tra s·ªë l∆∞·ª£ng vectors ƒë√£ upload
            collection_info = self.qdrant_client.get_collection(self.collection_name)
            print(f"S·ªë vectors trong collection: {collection_info.points_count}")
            
        except Exception as e:
            print(f"L·ªói khi upload embeddings: {str(e)}")
            raise e
    
    def run_full_pipeline(self,
                         data_source: str = "json",
                         data_path: str = "data/hasaki_db.products_info.json",
                         max_products: int = None,
                         chunk_size: int = DEFAULT_CHUNK_SIZE,
                         overlap: int = DEFAULT_OVERLAP,
                         markdown_chunk_size: int = MARKDOWN_CHUNK_SIZE,
                         markdown_overlap: int = MARKDOWN_OVERLAP,
                         include_markdown: bool = True,
                         save_backup: bool = True,
                         backup_path: str = "data/advanced_embeddings.json",
                         recreate_collection: bool = True,
                         upload_batch_size: int = 100):
        """
        Ch·∫°y to√†n b·ªô pipeline
        
        Args:
            data_source: Ngu·ªìn d·ªØ li·ªáu ("json" ho·∫∑c "mongodb")
            data_path: ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu
            max_products: S·ªë l∆∞·ª£ng s·∫£n ph·∫©m t·ªëi ƒëa
            chunk_size: K√≠ch th∆∞·ªõc chunk m·∫∑c ƒë·ªãnh
            overlap: ƒê·ªô ch·ªìng l·∫•p m·∫∑c ƒë·ªãnh
            markdown_chunk_size: K√≠ch th∆∞·ªõc chunk markdown
            markdown_overlap: ƒê·ªô ch·ªìng l·∫•p markdown
            include_markdown: C√≥ x·ª≠ l√Ω markdown kh√¥ng
            include_regular_description: C√≥ x·ª≠ l√Ω description th∆∞·ªùng kh√¥ng
            include_reviews: C√≥ x·ª≠ l√Ω reviews kh√¥ng
            include_comments: C√≥ x·ª≠ l√Ω comments kh√¥ng
            save_backup: C√≥ l∆∞u backup kh√¥ng
            backup_path: ƒê∆∞·ªùng d·∫´n file backup
            recreate_collection: C√≥ t·∫°o l·∫°i collection kh√¥ng
            upload_batch_size: K√≠ch th∆∞·ªõc batch upload
        """
        total_start_time = time.time()
        
        print("="*60)
        print("B·∫ÆT ƒê·∫¶U PIPELINE EMBEDDING N√ÇNG CAO")
        print("="*60)
        
        # 1. T·∫£i model
        if not self.model:
            self.load_model()
        
        # 2. T·∫£i v√† validate d·ªØ li·ªáu
        print("\n" + "-"*40)
        print("B∆Ø·ªöC 1: T·∫¢I V√Ä VALIDATE D·ªÆ LI·ªÜU")
        print("-"*40)
        
        products = load_and_validate_data(
            source=data_source,
            file_path=data_path,
            limit=max_products
        )
        
        if not products:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω!")
            return
        
        # 3. X·ª≠ l√Ω embeddings
        print("\n" + "-"*40)
        print("B∆Ø·ªöC 2: T·∫†O EMBEDDINGS")
        print("-"*40)
        
        embedding_start_time = time.time()
        
        embeddings = process_products_with_advanced_chunking(
            products=products,
            model=self.model,
            chunk_size=chunk_size,
            overlap=overlap,
            markdown_chunk_size=markdown_chunk_size,
            markdown_overlap=markdown_overlap,
            max_products=max_products,
            include_markdown=include_markdown
        )
        
        embedding_time = time.time() - embedding_start_time
        print(f"\nT·∫°o embeddings ho√†n th√†nh trong {embedding_time:.2f} gi√¢y")
        
        # 4. L∆∞u backup
        if save_backup and embeddings:
            print("\n" + "-"*40)
            print("B∆Ø·ªöC 3: L∆ØU BACKUP")
            print("-"*40)
            
            try:
                with open(backup_path, 'w', encoding='utf-8') as f:
                    json.dump(embeddings, f, ensure_ascii=False, indent=2)
                print(f"ƒê√£ l∆∞u backup v√†o: {backup_path}")
            except Exception as e:
                print(f"L·ªói khi l∆∞u backup: {str(e)}")
        
        # 5. K·∫øt n·ªëi Qdrant v√† upload
        print("\n" + "-"*40)
        print("B∆Ø·ªöC 4: UPLOAD L√äN QDRANT")
        print("-"*40)
        
        if not self.qdrant_client:
            self.connect_qdrant()
        
        self.setup_collection(recreate=recreate_collection)
        
        if embeddings:
            upload_start_time = time.time()
            self.upload_embeddings(embeddings, batch_size=upload_batch_size)
            upload_time = time.time() - upload_start_time
            print(f"Upload ho√†n th√†nh trong {upload_time:.2f} gi√¢y")
        
        # 6. T·ªïng k·∫øt
        total_time = time.time() - total_start_time
        
        print("\n" + "="*60)
        print("T·ªîNG K·∫æT PIPELINE")
        print("="*60)
        print(f"T·ªïng th·ªùi gian: {total_time:.2f} gi√¢y")
        print(f"S·ªë s·∫£n ph·∫©m x·ª≠ l√Ω: {len(products)}")
        print(f"S·ªë embeddings t·∫°o: {len(embeddings) if embeddings else 0}")
        print(f"Trung b√¨nh embeddings/s·∫£n ph·∫©m: {len(embeddings)/len(products):.1f}" if embeddings and products else "N/A")
        print(f"Collection: {self.collection_name}")
        print("="*60)

def main():
    """H√†m main ƒë·ªÉ ch·∫°y pipeline"""
    
    print("üöÄ HASAKI EMBEDDING PIPELINE")
    print("="*50)
    
    # L·ª±a ch·ªçn ngu·ªìn d·ªØ li·ªáu
    print("Ch·ªçn ngu·ªìn d·ªØ li·ªáu:")
    print("1. JSON file")
    print("2. MongoDB")
    
    choice = input("Nh·∫≠p l·ª±a ch·ªçn (1 ho·∫∑c 2): ").strip()
    
    if choice == "1":
        data_source = "json"
        data_path = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n file JSON (Enter ƒë·ªÉ d√πng m·∫∑c ƒë·ªãnh): ").strip()
        if not data_path:
            data_path = "data/hasaki_db.products_info.json"
    elif choice == "2":
        data_source = "mongodb"
        data_path = None
        print("S·ª≠ d·ª•ng MongoDB v·ªõi c·∫•u h√¨nh t·ª´ .env file")
    else:
        print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng JSON m·∫∑c ƒë·ªãnh")
        data_source = "json"
        data_path = "data/hasaki_db.products_info.json"
    
    # C·∫•u h√¨nh pipeline
    pipeline = AdvancedEmbeddingPipeline()
    
    # Ch·∫°y pipeline
    pipeline.run_full_pipeline(
        data_source=data_source,
        data_path=data_path,
        max_products=int(os.getenv("MAX_PRODUCTS", MAX_PRODUCTS_LIMIT)) if os.getenv("MAX_PRODUCTS") else MAX_PRODUCTS_LIMIT, 
        chunk_size=500,
        overlap=100,
        markdown_chunk_size=800,
        markdown_overlap=150,
        include_markdown=True,
        save_backup=True,
        backup_path="data/advanced_embeddings_markdown.json",
        recreate_collection=True,
        upload_batch_size=50
    )

if __name__ == "__main__":
    main()